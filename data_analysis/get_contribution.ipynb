{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../logs\\\\ABL_DROME_filter_log.csv', '../logs\\\\ACHA4_MOUSE_filter_log.csv', '../logs\\\\ANR17_HUMAN_filter_log.csv', '../logs\\\\CA2D3_MOUSE_filter_log.csv', '../logs\\\\CACB2_RABIT_filter_log.csv', '../logs\\\\CSKI1_MOUSE_filter_log.csv', '../logs\\\\DGLA_HUMAN_filter_log.csv', '../logs\\\\DOP1_HUMAN_filter_log.csv', '../logs\\\\GRIA2_filter_log.csv', '../logs\\\\IQEC1_HUMAN_filter_log.csv', '../logs\\\\K0513_MOUSE_filter_log.csv', '../logs\\\\KCNAS_DROME_filter_log.csv', '../logs\\\\MTUS2_HUMAN_filter_log.csv', '../logs\\\\PCLO_CHICK_filter_log.csv', '../logs\\\\PCLO_filter_log.csv', '../logs\\\\RIMS2_RAT_filter_log.csv', '../logs\\\\ROBO2_HUMAN_filter_log.csv', '../logs\\\\RUSC2_MOUSE_filter_log.csv', '../logs\\\\SCN1_HETBL_filter_log.csv', '../logs\\\\TRIM2_BOVIN_filter_log.csv', '../logs\\\\TWK7_CAEEL_filter_log.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path = '../logs'\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if f.endswith('.csv')]\n",
    "files = [os.path.join(path, f) for f in files]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contribution_per_step(file):\n",
    "    df = pd.read_csv(file)\n",
    "    # Track changes for each row in the dataframe\n",
    "    df['change_working'] = df['Working'].diff()\n",
    "    df['change_picked'] = df['Picked'].diff()\n",
    "    df['change_x_count'] = df['X count'].diff()\n",
    "\n",
    "    # Create new columns to store cumulative changes\n",
    "    df['cumulative_working'] = df['change_working']\n",
    "    df['cumulative_picked'] = df['change_picked']\n",
    "    df['cumulative_x_count'] = df['change_x_count']\n",
    "\n",
    "    # Iterate over the dataframe and apply the merging logic\n",
    "    for i in range(1, len(df)):\n",
    "        needs_to_merge = df.loc[i, 'Method'] in ['Pick Must Have Assignments', 'Merge Lonely Sequences', 'Assign Concensous for Isolated']\n",
    "        if needs_to_merge:\n",
    "            df.loc[i, 'cumulative_working'] += df.loc[i+1, 'cumulative_working']\n",
    "            df.loc[i, 'cumulative_picked'] += df.loc[i+1, 'cumulative_picked']\n",
    "            df.loc[i, 'cumulative_x_count'] += df.loc[i+1, 'cumulative_x_count']\n",
    "            df.loc[i+1, 'cumulative_working'] = 0\n",
    "            df.loc[i+1, 'cumulative_picked'] = 0\n",
    "            df.loc[i+1, 'cumulative_x_count'] = 0\n",
    "\n",
    "    # Get names of all methods\n",
    "    methods = df['Method'].unique()\n",
    "\n",
    "    # Make a dictionary that goes from method to change in each metric\n",
    "    method_to_change = {}\n",
    "    for method in methods:\n",
    "        method_df = df[df['Method'] == method]\n",
    "        method_to_change[method] = (\n",
    "            method_df['cumulative_working'].sum(),\n",
    "            method_df['cumulative_picked'].sum(),\n",
    "            method_df['cumulative_x_count'].sum()\n",
    "        )\n",
    "\n",
    "    return method_to_change\n",
    "\n",
    "def get_initial(file):\n",
    "    df = pd.read_csv(file)\n",
    "    initial_working = df.loc[0, 'Working']\n",
    "    initial_x_count = df.loc[0, 'X count']\n",
    "\n",
    "    return initial_working, initial_x_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working: 421238, Picked: 0, X count: 2875125 in ../logs\\ABL_DROME_filter_log.csv\n",
      "Working: 603862, Picked: 0, X count: 3881367 in ../logs\\ACHA4_MOUSE_filter_log.csv\n",
      "Working: 242745, Picked: 0, X count: 1709832 in ../logs\\ANR17_HUMAN_filter_log.csv\n",
      "Working: 316931, Picked: 0, X count: 1148215 in ../logs\\CA2D3_MOUSE_filter_log.csv\n",
      "Working: 379584, Picked: 0, X count: 1742568 in ../logs\\CACB2_RABIT_filter_log.csv\n",
      "Working: 759863, Picked: 0, X count: 9300973 in ../logs\\CSKI1_MOUSE_filter_log.csv\n",
      "Working: 309182, Picked: 0, X count: 2693960 in ../logs\\DGLA_HUMAN_filter_log.csv\n",
      "Working: 483561, Picked: 0, X count: 3453449 in ../logs\\DOP1_HUMAN_filter_log.csv\n",
      "Working: 52953, Picked: 0, X count: 960482 in ../logs\\GRIA2_filter_log.csv\n",
      "Working: 536053, Picked: 0, X count: 4487167 in ../logs\\IQEC1_HUMAN_filter_log.csv\n",
      "Working: 490892, Picked: 0, X count: 1748602 in ../logs\\K0513_MOUSE_filter_log.csv\n",
      "Working: 250919, Picked: 0, X count: 1374529 in ../logs\\KCNAS_DROME_filter_log.csv\n",
      "Working: 588768, Picked: 0, X count: 3854901 in ../logs\\MTUS2_HUMAN_filter_log.csv\n",
      "Working: 632294, Picked: 0, X count: 6653920 in ../logs\\PCLO_CHICK_filter_log.csv\n",
      "Working: 52619, Picked: 0, X count: 1068125 in ../logs\\PCLO_filter_log.csv\n",
      "Working: 638769, Picked: 0, X count: 6554846 in ../logs\\RIMS2_RAT_filter_log.csv\n",
      "Working: 1040589, Picked: 0, X count: 9858534 in ../logs\\ROBO2_HUMAN_filter_log.csv\n",
      "Working: 212926, Picked: 0, X count: 1051278 in ../logs\\RUSC2_MOUSE_filter_log.csv\n",
      "Working: 333336, Picked: 0, X count: 3473784 in ../logs\\SCN1_HETBL_filter_log.csv\n",
      "Working: 189297, Picked: 0, X count: 1235089 in ../logs\\TRIM2_BOVIN_filter_log.csv\n",
      "Working: 510909, Picked: 0, X count: 2974417 in ../logs\\TWK7_CAEEL_filter_log.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    working= df['Working'].iloc[0]\n",
    "    picked= df['Picked'].iloc[0]\n",
    "    x_count= df['X count'].iloc[0]\n",
    "    print(f'Working: {working}, Picked: {picked}, X count: {x_count} in {file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_pie_charts(method_to_change, thresholds=[5, 5, 5], show_legend=True, font_size=10, legend_font_size=12, custom_names=None, initial_working=None, initial_x_count=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Creates multiple pie charts from the method_to_change dictionary and arranges them side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - method_to_change (dict): Dictionary with methods as keys and tuples of changes as values.\n",
    "    - thresholds (list of floats): Percentage thresholds below which percentages are not shown.\n",
    "    - show_legend (bool): Whether to show the legend or not.\n",
    "    - font_size (int): Font size for the pie chart labels.\n",
    "    - legend_font_size (int): Font size for the legend text.\n",
    "    - custom_names (dict): Dictionary to map original method names to custom names for the legend.\n",
    "    - total (float): The total value that the pie chart should represent. If provided, the difference will be labeled as \"Uncovered\".\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    method_names = ['Working', 'Picked', 'X Count']\n",
    "\n",
    "    # Generate a color map for each label\n",
    "    labels = list(method_to_change.keys())\n",
    "    colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "    color_map = {label: colors[i % len(colors)] for i, label in enumerate(labels)}\n",
    "    color_map['Uncovered'] = colors[(len(labels) + 1) % len(colors)]  # Add \"Uncovered\" to color map\n",
    "    #color_map['Leftover'] = colors[(len(labels) + 2) % len(colors)]  # Add \"Leftover\" to color map\n",
    "    color_map['Leftover'] = colors[(len(labels) + 1) % len(colors)]  # Add \"Uncovered\" to color map but with the same color as \"Uncovered\"\n",
    "\n",
    "\n",
    "    for i, method_n in enumerate([0, 2, 1]):\n",
    "        # Extract labels and sizes\n",
    "        if method_n in [0, 2]:\n",
    "            sizes = [-method_to_change[method][method_n] for method in method_to_change]\n",
    "        else:\n",
    "            sizes = [method_to_change[method][method_n] for method in method_to_change]\n",
    "        \n",
    "        total_size = sum(sizes)\n",
    "        sizes_with_labels = [(size, label) for size, label in zip(sizes, labels)]\n",
    "        \n",
    "        # Filter out slices that round to 0.0%\n",
    "        filtered_sizes, filtered_labels = zip(\n",
    "            *[(size, label) for size, label in sizes_with_labels if size != 0]\n",
    "        )\n",
    "        \n",
    "        # If total is provided and sizes don't add up to total, add \"Uncovered\" slice\n",
    "        if method_n == 0 and initial_working is not None and total_size < initial_working:\n",
    "            uncovered_size = initial_working - total_size\n",
    "            filtered_sizes = list(filtered_sizes) + [uncovered_size]\n",
    "            filtered_labels = list(filtered_labels) + ['Uncovered']\n",
    "\n",
    "        if method_n == 2 and initial_x_count is not None and total_size < initial_x_count:\n",
    "            uncovered_size = initial_x_count - total_size\n",
    "            filtered_sizes = list(filtered_sizes) + [uncovered_size]\n",
    "            filtered_labels = list(filtered_labels) + ['Leftover']\n",
    "        \n",
    "        # Create pie chart with consistent colors\n",
    "        pie_colors = [color_map[label] for label in filtered_labels]\n",
    "        \n",
    "        wedges, texts, autotexts = axes[i].pie(\n",
    "            filtered_sizes, labels=None, autopct=lambda p: f'{p:.1f}%' if p >= thresholds[i] else '', startangle=140, colors=pie_colors,\n",
    "            textprops={'fontsize': font_size}, pctdistance=0.8\n",
    "        )\n",
    "\n",
    "        axes[i].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "    # Optionally add a common legend\n",
    "    if show_legend:\n",
    "        # Apply custom names to labels if provided\n",
    "        if custom_names:\n",
    "            formatted_labels = [custom_names.get(label, label) for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        else:\n",
    "            formatted_labels = [' '.join([word.lower() if idx > 0 else word for idx, word in enumerate(label.split())]) for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        \n",
    "        handles = [plt.Line2D([0], [0], color=color_map[label], marker='o', linestyle='') for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        fig.legend(handles, formatted_labels, title=None, loc=\"center right\", bbox_to_anchor=(0.95, 1.3), ncol=2, prop={'size': legend_font_size})\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def create_horizontal_bar_plots(\n",
    "    method_to_change,\n",
    "    thresholds=[5, 5, 5],\n",
    "    show_legend=True,\n",
    "    font_size=10,\n",
    "    legend_font_size=12,\n",
    "    custom_names=None,\n",
    "    initial_working=None,\n",
    "    initial_x_count=None,\n",
    "    save_path=None,\n",
    "    show_titles=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates horizontal bar plots where each bar represents 100% with contributions from different methods.\n",
    "\n",
    "    Parameters:\n",
    "    - method_to_change (dict): Dictionary with methods as keys and tuples of changes as values.\n",
    "    - thresholds (list of floats): Percentage thresholds below which percentages are not shown.\n",
    "    - show_legend (bool): Whether to show the legend or not.\n",
    "    - font_size (int): Font size for the bar chart labels.\n",
    "    - legend_font_size (int): Font size for the legend text.\n",
    "    - custom_names (dict): Dictionary to map original method names to custom names for the legend.\n",
    "    - initial_working (float): The initial working value for comparison.\n",
    "    - initial_x_count (float): The initial x count value for comparison.\n",
    "    - save_path (str): Path to save the plot if provided.\n",
    "    - show_titles (bool): Whether to show the titles for each plot (default is False).\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 2))  # Arrange bars side by side\n",
    "    method_names = ['Working', 'Picked', 'X Count']\n",
    "\n",
    "    # Generate a color map for each label\n",
    "    labels = list(method_to_change.keys())\n",
    "    colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "    color_map = {label: colors[i % len(colors)] for i, label in enumerate(labels)}\n",
    "    color_map['Uncovered'] = colors[(len(labels) + 1) % len(colors)]\n",
    "    color_map['Leftover'] = colors[(len(labels) + 1) % len(colors)]\n",
    "\n",
    "    for i, method_n in enumerate([0, 2, 1]):\n",
    "        # Extract sizes\n",
    "        if method_n in [0, 2]:\n",
    "            sizes = [-method_to_change[method][method_n] for method in method_to_change]\n",
    "        else:\n",
    "            sizes = [method_to_change[method][method_n] for method in method_to_change]\n",
    "        \n",
    "        total_size = sum(sizes)\n",
    "        sizes_with_labels = [(size, label) for size, label in zip(sizes, labels)]\n",
    "        \n",
    "        # Filter out slices that round to 0.0%\n",
    "        filtered_sizes, filtered_labels = zip(\n",
    "            *[(size, label) for size, label in sizes_with_labels if size != 0]\n",
    "        )\n",
    "\n",
    "        # Normalize sizes to represent percentages of 100%\n",
    "        if method_n == 0:\n",
    "            percentages = [size / initial_working * 100 for size in filtered_sizes]\n",
    "        elif method_n == 2:\n",
    "            percentages = [size / initial_x_count * 100 for size in filtered_sizes]\n",
    "        else:\n",
    "            percentages = [size / total_size * 100 for size in filtered_sizes]\n",
    "\n",
    "        #print(total_size/initial_x_count*100)\n",
    "\n",
    "        # Adjust to include \"Uncovered\" or \"Leftover\" without exceeding 100%\n",
    "        if method_n == 0 and initial_working is not None and total_size < initial_working:\n",
    "            uncovered_size = initial_working - total_size\n",
    "            uncovered_percent = uncovered_size / initial_working * 100\n",
    "            percentages.append(uncovered_percent)\n",
    "            filtered_labels = list(filtered_labels) + ['Uncovered']\n",
    "\n",
    "        if method_n == 2 and initial_x_count is not None and total_size < initial_x_count:\n",
    "            leftover_size = initial_x_count - total_size\n",
    "            leftover_percent = leftover_size / initial_x_count * 100\n",
    "            percentages.append(leftover_percent)\n",
    "            filtered_labels = list(filtered_labels) + ['Leftover']\n",
    "\n",
    "        # Normalize again if total percentage exceeds 100%\n",
    "        total_percentage = sum(percentages)\n",
    "        if total_percentage > 100:\n",
    "            percentages = [p / total_percentage * 100 for p in percentages]\n",
    "\n",
    "        # Sort contributions by size for ordered display\n",
    "        sorted_indices = sorted(range(len(percentages)), key=lambda k: percentages[k], reverse=True)\n",
    "        sorted_percentages = [percentages[j] for j in sorted_indices]\n",
    "        sorted_labels = [filtered_labels[j] for j in sorted_indices]\n",
    "        sorted_colors = [color_map[label] for label in sorted_labels]\n",
    "\n",
    "        # Create a single horizontal bar plot with contributions represented by different colors\n",
    "        left = 0\n",
    "        \n",
    "        for j, (percent, color) in enumerate(zip(sorted_percentages, sorted_colors)):\n",
    "            axes[i].barh(0, percent, left=left, color=color, height=0.1)  # Adjust height for thinner bars\n",
    "            #print(f'{method_names[i]}: {sorted_labels[j]}: {percent:.1f}%')\n",
    "            if percent >= thresholds[i]:\n",
    "                # Round the percentage to one decimal place\n",
    "                rounded_percent = round(percent, 1)\n",
    "\n",
    "\n",
    "                # Update the text to display the rounded percentage\n",
    "                axes[i].text(\n",
    "                    left + rounded_percent / 2,\n",
    "                    0,\n",
    "                    f'{rounded_percent:.1f}%',\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    fontsize=font_size,\n",
    "                    color='black',\n",
    "                    rotation=90\n",
    "                )\n",
    "            left += percent\n",
    "\n",
    "        axes[i].set_xlim(0, 100)\n",
    "        axes[i].set_yticks([])  # Hide y-axis ticks\n",
    "        axes[i].axis('off')  # Remove axis and boxes\n",
    "\n",
    "        if show_titles:  # Only show titles if show_titles is True\n",
    "            axes[i].set_title(method_names[i], fontsize=font_size)\n",
    "\n",
    "    # Optionally add a common legend\n",
    "    if show_legend:\n",
    "        if custom_names:\n",
    "            formatted_labels = [custom_names.get(label, label) for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        else:\n",
    "            formatted_labels = [' '.join([word.lower() if idx > 0 else word for idx, word in enumerate(label.split())]) for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        \n",
    "        handles = [plt.Line2D([0], [0], color=color_map[label], marker='o', linestyle='') for label in list(set(labels) - set(['Initial'])) + ['Uncovered', 'Leftover']]\n",
    "        fig.legend(handles, formatted_labels, title=None, loc=\"center right\", bbox_to_anchor=(1.1, 0.5), ncol=1, prop={'size': legend_font_size})\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_names = {\n",
    "    'Pick Must Have Assignments': 'Adding sequence assignments that cover multiple reads',\n",
    "    'Merge Lonely Sequences': 'Merging reads which agree with only one other reads',\n",
    "    'Apply Local Concensous': 'Assigning local consensus',\n",
    "    'Assign Concensous for Isolated': 'Assigning consensus for isolated reads',\n",
    "    'Update Knowns': 'Adding non-degenerate reads',\n",
    "    'Remove Less Specific': 'Removing less-specific reads',\n",
    "    'Others': 'Other'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Initial': (0.0, 0.0, 0.0), 'Update Knowns': (-4627.0, 160.0, -85136.0), 'Remove Less Specific': (-17878.0, 0.0, -355964.0), 'Pick Must Have Assignments': (-712.0, 165.0, -5960.0), 'Merge Lonely Sequences': (-2193.0, 101.0, -61654.0), 'Assign Concensous for Isolated': (-6745.0, 6745.0, -83775.0), 'Apply Local Concensous': (0.0, 0.0, -33590.0)}\n",
      "3.3477982929404195\n",
      "Working: Uncovered: 39.3%\n",
      "Working: Remove Less Specific: 33.8%\n",
      "Working: Assign Concensous for Isolated: 12.7%\n",
      "Working: Update Knowns: 8.7%\n",
      "Working: Merge Lonely Sequences: 4.1%\n",
      "Working: Pick Must Have Assignments: 1.3%\n",
      "65.18383478295272\n",
      "Picked: Remove Less Specific: 37.1%\n",
      "Picked: Leftover: 34.8%\n",
      "Picked: Update Knowns: 8.9%\n",
      "Picked: Assign Concensous for Isolated: 8.7%\n",
      "Picked: Merge Lonely Sequences: 6.4%\n",
      "Picked: Apply Local Concensous: 3.5%\n",
      "Picked: Pick Must Have Assignments: 0.6%\n",
      "0.7466043090864796\n",
      "X Count: Assign Concensous for Isolated: 94.1%\n",
      "X Count: Pick Must Have Assignments: 2.3%\n",
      "X Count: Update Knowns: 2.2%\n",
      "X Count: Merge Lonely Sequences: 1.4%\n",
      "4.414183733177297\n",
      "Working: Assign Concensous for Isolated: 47.3%\n",
      "Working: Remove Less Specific: 25.2%\n",
      "Working: Uncovered: 10.4%\n",
      "Working: Update Knowns: 8.6%\n",
      "Working: Merge Lonely Sequences: 8.2%\n",
      "Working: Pick Must Have Assignments: 0.4%\n",
      "91.28866003510825\n",
      "Picked: Assign Concensous for Isolated: 33.7%\n",
      "Picked: Remove Less Specific: 28.5%\n",
      "Picked: Merge Lonely Sequences: 13.6%\n",
      "Picked: Update Knowns: 8.8%\n",
      "Picked: Leftover: 8.7%\n",
      "Picked: Apply Local Concensous: 6.5%\n",
      "Picked: Pick Must Have Assignments: 0.1%\n",
      "2.358057343475717\n",
      "X Count: Assign Concensous for Isolated: 98.7%\n",
      "X Count: Update Knowns: 0.6%\n",
      "X Count: Merge Lonely Sequences: 0.4%\n",
      "X Count: Pick Must Have Assignments: 0.3%\n",
      "{'Initial': (0.0, 0.0, 0.0), 'Update Knowns': (-4506.0, 162.0, -94443.0), 'Remove Less Specific': (-13276.0, 0.0, -304336.0), 'Pick Must Have Assignments': (-204.0, 67.0, -1584.0), 'Merge Lonely Sequences': (-4295.0, 90.0, -145139.0), 'Assign Concensous for Isolated': (-24868.0, 24868.0, -360260.0), 'Apply Local Concensous': (0.0, 0.0, -69315.0)}\n"
     ]
    }
   ],
   "source": [
    "method_to_change = get_contribution_per_step('../logs\\\\GRIA2_filter_log.csv')\n",
    "initial_working, initial_x_count = get_initial('../logs\\\\GRIA2_filter_log.csv')\n",
    "print(method_to_change)\n",
    "#create_pie_charts(method_to_change, thresholds=[3, 3, 3], show_legend=False, custom_names=custom_names, font_size=22, legend_font_size=20, initial_working=initial_working, initial_x_count=initial_x_count, save_path='GRIA2_filter.svg')\n",
    "create_horizontal_bar_plots(method_to_change, thresholds=[5, 5, 5], show_legend=False, custom_names=custom_names, font_size=22, legend_font_size=20, initial_working=initial_working, initial_x_count=initial_x_count, save_path='GRIA2_filter_bar.svg')\n",
    "#create_pie_charts(method_to_change, thresholds=[3, 3, 3], show_legend=True, custom_names=custom_names, font_size=22, legend_font_size=20, initial_working=initial_working, initial_x_count=initial_x_count)\n",
    "\n",
    "method_to_change = get_contribution_per_step('../logs\\\\PCLO_filter_log.csv')\n",
    "initial_working, initial_x_count = get_initial('../logs\\\\PCLO_filter_log.csv')\n",
    "#create_pie_charts(method_to_change, thresholds=[3, 3, 3], show_legend=False, font_size=22,initial_working=initial_working, initial_x_count=initial_x_count, save_path='PCLO_filter.svg')\n",
    "create_horizontal_bar_plots(method_to_change, thresholds=[5, 5, 5], show_legend=False, font_size=22, initial_working=initial_working, initial_x_count=initial_x_count, save_path='PCLO_filter_bar.svg')\n",
    "print(method_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs\\TRIM2_BOVIN_filter_log.csv\n",
      "15.250398959103354\n",
      "Working: Update Knowns: 69.8%\n",
      "Working: Assign Concensous for Isolated: 8.7%\n",
      "Working: Remove Less Specific: 8.7%\n",
      "Working: Pick Must Have Assignments: 8.1%\n",
      "Working: Merge Lonely Sequences: 4.2%\n",
      "Working: Uncovered: 0.5%\n",
      "99.6575145596795\n",
      "Picked: Update Knowns: 72.7%\n",
      "Picked: Remove Less Specific: 9.8%\n",
      "Picked: Pick Must Have Assignments: 4.8%\n",
      "Picked: Merge Lonely Sequences: 4.8%\n",
      "Picked: Assign Concensous for Isolated: 4.3%\n",
      "Picked: Apply Local Concensous: 3.3%\n",
      "Picked: Leftover: 0.3%\n",
      "2.508078365202832\n",
      "X Count: Assign Concensous for Isolated: 53.3%\n",
      "X Count: Pick Must Have Assignments: 21.2%\n",
      "X Count: Update Knowns: 16.5%\n",
      "X Count: Merge Lonely Sequences: 9.0%\n",
      "../logs\\TWK7_CAEEL_filter_log.csv\n",
      "17.17264257163673\n",
      "Working: Update Knowns: 93.8%\n",
      "Working: Assign Concensous for Isolated: 2.5%\n",
      "Working: Pick Must Have Assignments: 1.6%\n",
      "Working: Remove Less Specific: 1.4%\n",
      "Working: Merge Lonely Sequences: 0.6%\n",
      "Working: Uncovered: 0.0%\n",
      "99.98769506763846\n",
      "Picked: Update Knowns: 95.3%\n",
      "Picked: Remove Less Specific: 1.3%\n",
      "Picked: Assign Concensous for Isolated: 1.2%\n",
      "Picked: Apply Local Concensous: 0.9%\n",
      "Picked: Pick Must Have Assignments: 0.7%\n",
      "Picked: Merge Lonely Sequences: 0.6%\n",
      "Picked: Leftover: 0.0%\n",
      "1.088751173759429\n",
      "X Count: Update Knowns: 39.9%\n",
      "X Count: Assign Concensous for Isolated: 39.3%\n",
      "X Count: Pick Must Have Assignments: 16.0%\n",
      "X Count: Merge Lonely Sequences: 4.8%\n",
      "../logs\\PCLO_CHICK_filter_log.csv\n",
      "9.439037439584485\n",
      "Working: Update Knowns: 54.5%\n",
      "Working: Remove Less Specific: 39.6%\n",
      "Working: Assign Concensous for Isolated: 2.9%\n",
      "Working: Pick Must Have Assignments: 1.2%\n",
      "Working: Merge Lonely Sequences: 1.1%\n",
      "Working: Uncovered: 0.7%\n",
      "99.6804440089451\n",
      "Picked: Update Knowns: 59.3%\n",
      "Picked: Remove Less Specific: 36.5%\n",
      "Picked: Assign Concensous for Isolated: 1.2%\n",
      "Picked: Apply Local Concensous: 1.0%\n",
      "Picked: Merge Lonely Sequences: 1.0%\n",
      "Picked: Pick Must Have Assignments: 0.7%\n",
      "Picked: Leftover: 0.3%\n",
      "0.376244379253132\n",
      "X Count: Assign Concensous for Isolated: 72.3%\n",
      "X Count: Merge Lonely Sequences: 10.1%\n",
      "X Count: Pick Must Have Assignments: 9.4%\n",
      "X Count: Update Knowns: 8.2%\n",
      "../logs\\ABL_DROME_filter_log.csv\n",
      "14.640789530889961\n",
      "Working: Update Knowns: 70.2%\n",
      "Working: Remove Less Specific: 13.3%\n",
      "Working: Assign Concensous for Isolated: 12.7%\n",
      "Working: Merge Lonely Sequences: 2.5%\n",
      "Working: Pick Must Have Assignments: 1.3%\n",
      "Working: Uncovered: 0.1%\n",
      "99.96361897308813\n",
      "Picked: Update Knowns: 77.3%\n",
      "Picked: Remove Less Specific: 11.4%\n",
      "Picked: Assign Concensous for Isolated: 5.1%\n",
      "Picked: Merge Lonely Sequences: 3.0%\n",
      "Picked: Apply Local Concensous: 2.6%\n",
      "Picked: Pick Must Have Assignments: 0.6%\n",
      "Picked: Leftover: 0.0%\n",
      "2.3603495500195644\n",
      "X Count: Assign Concensous for Isolated: 78.8%\n",
      "X Count: Update Knowns: 8.5%\n",
      "X Count: Merge Lonely Sequences: 7.5%\n",
      "X Count: Pick Must Have Assignments: 5.2%\n",
      "../logs\\KCNAS_DROME_filter_log.csv\n",
      "18.253161628456002\n",
      "Working: Update Knowns: 94.9%\n",
      "Working: Remove Less Specific: 3.4%\n",
      "Working: Assign Concensous for Isolated: 0.9%\n",
      "Working: Pick Must Have Assignments: 0.6%\n",
      "Working: Merge Lonely Sequences: 0.3%\n",
      "Working: Uncovered: 0.0%\n",
      "99.99548936399304\n",
      "Picked: Update Knowns: 96.6%\n",
      "Picked: Remove Less Specific: 2.5%\n",
      "Picked: Assign Concensous for Isolated: 0.3%\n",
      "Picked: Merge Lonely Sequences: 0.2%\n",
      "Picked: Pick Must Have Assignments: 0.2%\n",
      "Picked: Apply Local Concensous: 0.2%\n",
      "Picked: Leftover: 0.0%\n",
      "0.33793393955311235\n",
      "X Count: Assign Concensous for Isolated: 47.0%\n",
      "X Count: Update Knowns: 27.5%\n",
      "X Count: Pick Must Have Assignments: 19.6%\n",
      "X Count: Merge Lonely Sequences: 5.9%\n",
      "../logs\\SCN1_HETBL_filter_log.csv\n",
      "9.450961832975223\n",
      "Working: Update Knowns: 60.1%\n",
      "Working: Remove Less Specific: 29.0%\n",
      "Working: Assign Concensous for Isolated: 4.1%\n",
      "Working: Pick Must Have Assignments: 3.8%\n",
      "Working: Merge Lonely Sequences: 1.6%\n",
      "Working: Uncovered: 1.5%\n",
      "99.04637133454469\n",
      "Picked: Update Knowns: 62.9%\n",
      "Picked: Remove Less Specific: 28.1%\n",
      "Picked: Pick Must Have Assignments: 2.3%\n",
      "Picked: Apply Local Concensous: 2.2%\n",
      "Picked: Merge Lonely Sequences: 1.9%\n",
      "Picked: Assign Concensous for Isolated: 1.7%\n",
      "Picked: Leftover: 1.0%\n",
      "0.5444495109655637\n",
      "X Count: Assign Concensous for Isolated: 71.8%\n",
      "X Count: Pick Must Have Assignments: 14.9%\n",
      "X Count: Update Knowns: 8.0%\n",
      "X Count: Merge Lonely Sequences: 5.4%\n",
      "../logs\\ANR17_HUMAN_filter_log.csv\n",
      "14.19548821170735\n",
      "Working: Update Knowns: 94.2%\n",
      "Working: Assign Concensous for Isolated: 2.9%\n",
      "Working: Remove Less Specific: 2.1%\n",
      "Working: Merge Lonely Sequences: 0.6%\n",
      "Working: Pick Must Have Assignments: 0.2%\n",
      "Working: Uncovered: 0.0%\n",
      "99.9938005605229\n",
      "Picked: Update Knowns: 96.8%\n",
      "Picked: Remove Less Specific: 1.6%\n",
      "Picked: Assign Concensous for Isolated: 0.9%\n",
      "Picked: Merge Lonely Sequences: 0.5%\n",
      "Picked: Apply Local Concensous: 0.3%\n",
      "Picked: Pick Must Have Assignments: 0.0%\n",
      "Picked: Leftover: 0.0%\n",
      "0.5783024297123929\n",
      "X Count: Assign Concensous for Isolated: 72.1%\n",
      "X Count: Update Knowns: 14.8%\n",
      "X Count: Merge Lonely Sequences: 9.8%\n",
      "X Count: Pick Must Have Assignments: 3.3%\n",
      "../logs\\DGLA_HUMAN_filter_log.csv\n",
      "11.442634634515732\n",
      "Working: Update Knowns: 63.5%\n",
      "Working: Remove Less Specific: 21.4%\n",
      "Working: Assign Concensous for Isolated: 9.2%\n",
      "Working: Pick Must Have Assignments: 3.0%\n",
      "Working: Merge Lonely Sequences: 2.6%\n",
      "Working: Uncovered: 0.3%\n",
      "99.84944097165511\n",
      "Picked: Update Knowns: 68.9%\n",
      "Picked: Remove Less Specific: 19.1%\n",
      "Picked: Apply Local Concensous: 3.8%\n",
      "Picked: Merge Lonely Sequences: 3.3%\n",
      "Picked: Assign Concensous for Isolated: 3.1%\n",
      "Picked: Pick Must Have Assignments: 1.5%\n",
      "Picked: Leftover: 0.2%\n",
      "1.378602503377927\n",
      "X Count: Assign Concensous for Isolated: 76.7%\n",
      "X Count: Pick Must Have Assignments: 9.6%\n",
      "X Count: Update Knowns: 8.1%\n",
      "X Count: Merge Lonely Sequences: 5.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#print(sorted_short_reads)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_short_reads):\n\u001b[1;32m---> 12\u001b[0m     method_to_change \u001b[38;5;241m=\u001b[39m \u001b[43mget_contribution_per_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     initial_working, initial_x_count \u001b[38;5;241m=\u001b[39m get_initial(file)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mget_contribution_per_step\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_merge:\n\u001b[0;32m     17\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_working\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_working\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcumulative_picked\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_picked\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_x_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_x_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_working\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1835\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1835\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1928\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1928\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:2034\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2032\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 2034\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1385\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1384\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1385\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1159\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1157\u001b[0m         value \u001b[38;5;241m=\u001b[39m ensure_block_shape(value, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m   1160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of new values must be compatible with manager shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1162\u001b[0m         )\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(loc):\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;66;03m# We have 6 tests where loc is _not_ an int.\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;66;03m# In this case, get_blkno_placements will yield only one tuple,\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;66;03m#  containing (self._blknos[loc], BlockPlacement(slice(0, 1, 1)))\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# Check if we can use _iset_single fastpath\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:56\u001b[0m, in \u001b[0;36mDataManager.shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Shape:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\tzion\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:56\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Shape:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "short_reads = list(set(files)-  set(['../logs\\\\GRIA2_filter_log.csv', '../logs\\\\PCLO_filter_log.csv']))\n",
    "def sorting_key(x):\n",
    "    species = x.split('_')[1]\n",
    "    name = x.split('_')[0].split('\\\\')[1]\n",
    "    return (species, name)\n",
    "\n",
    "sorted_short_reads = sorted(short_reads, key=sorting_key)\n",
    "#print(sorted_short_reads)\n",
    "\n",
    "\n",
    "for i, file in enumerate(sorted_short_reads):\n",
    "    method_to_change = get_contribution_per_step(file)\n",
    "    initial_working, initial_x_count = get_initial(file)\n",
    "    print(file)\n",
    "    #print(method_to_change)\n",
    "    #create_pie_charts(method_to_change, thresholds=[5,5, 5], show_legend=(i==0), save_path=f'plots/{i}.svg', custom_names=custom_names, font_size=22, legend_font_size=18, initial_working=initial_working, initial_x_count=initial_x_count)\n",
    "    create_horizontal_bar_plots(method_to_change, thresholds=[5, 5, 5], show_legend=False, custom_names=custom_names, font_size=22, legend_font_size=20, initial_working=initial_working, initial_x_count=initial_x_count, save_path=f'plots/{i}_bar.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
